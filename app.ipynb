{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:52:10.107438Z",
     "start_time": "2025-03-02T21:51:48.580231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy\n",
    "import cv2\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from deepface import DeepFace\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from transformers import pipeline\n",
    "import whisper\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "emotion_model = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "topic_model = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ],
   "id": "a1f762a9db141d43",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:52:10.154266Z",
     "start_time": "2025-03-02T21:52:10.121590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def extract_frames_gray_scale(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(\"actor cap\",cap)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(\"actor fps: \",fps)\n",
    "    \n",
    "    # Pobranie wymiarów pierwszej klatki\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        cap.release()\n",
    "        raise ValueError(\"Nie można odczytać pierwszej klatki wideo!\")\n",
    "\n",
    "    # height, width, channels = first_frame.shape  # Pobranie wymiarów obrazu\n",
    "    print(\"actor 1st frame\",first_frame.shape)\n",
    "    # Konwersja pierwszej klatki na grayscale i dodanie trzeciego wymiaru\n",
    "    first_frame_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    frames = [np.expand_dims(first_frame_gray, axis=-1)]  # dodajemy kanał, by mieć (wysokość, szerokość, 1)\n",
    "    frame_idx = fps  # Pierwsza klatka już dodana\n",
    "    while cap.isOpened():\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Konwersja do grayscale i dodanie kanału\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frames.append(np.expand_dims(gray_frame, axis=-1))  # dodajemy kanał, by mieć (wysokość, szerokość, 1)\n",
    "        frame_idx += fps\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def extract_frames_rgb(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(\"critic fps: \",fps)\n",
    "    \n",
    "    # Pobranie pierwszej klatki wideo\n",
    "    ret, first_frame = cap.read()\n",
    "    print(\"critic 1st frame\",first_frame.shape)\n",
    "    if not ret:\n",
    "        cap.release()\n",
    "        raise ValueError(\"Nie można odczytać pierwszej klatki wideo!\")\n",
    "    \n",
    "    frames_rgb = [cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB)]\n",
    "    \n",
    "\n",
    "    frame_idx = fps  # Pomijamy pierwszą klatkę, bo już ją dodaliśmy\n",
    "    while cap.isOpened():\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames_rgb.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Konwersja BGR → RGB\n",
    "        frame_idx += fps\n",
    "    \n",
    "    cap.release()\n",
    "    return frames_rgb\n",
    "\n",
    "\n",
    "#nagroda, gotowe clpy\n",
    "\n",
    "def calculate_video_score(frames, clip_length, skip):\n",
    "    emotion_scores = []\n",
    "    \n",
    "\n",
    "    \n",
    "    emotion_intensity = {\n",
    "                'happy': 0.7,\n",
    "                'surprise': 0.95,\n",
    "                'angry': 0.9,\n",
    "                'sad': 0.4,\n",
    "                'neutral': 0.1,\n",
    "                'disgust': 0.9,\n",
    "                'face_confidence': 0.1,\n",
    "                'fear': 0.95\n",
    "            }\n",
    "    \n",
    "    \n",
    "    # emotion_intensity = {\n",
    "    #             'happy': 0.7,\n",
    "    #             'surprise': -0.95,\n",
    "    #             'angry': -0.9,\n",
    "    #             'sad': -0.4,\n",
    "    #             'neutral': -0.1,\n",
    "    #             'disgust': -0.9,\n",
    "    #             'face_confidence': -0.1,\n",
    "    #             'fear': -0.95\n",
    "    #         }\n",
    "    \n",
    "    for frame in frames:\n",
    "            try:\n",
    "                analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False, detector_backend='opencv')\n",
    "        \n",
    "                emotion_values = analysis[0]['emotion']\n",
    "                \n",
    "                normalized_emotions = {emotion: value / 100 for emotion, value in emotion_values.items()}\n",
    "                \n",
    "                sorted_emotions = sorted(normalized_emotions.items(), key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "        \n",
    "                threshold = 0.5\n",
    "        \n",
    "                top_3_emotions = [(emotion, value) for emotion, value in sorted_emotions[:3] if value > threshold]\n",
    "        \n",
    "                # Jeśli mamy mniej niż 3 emocje powyżej progu, wstawiamy 0 dla brakujących\n",
    "                while len(top_3_emotions) < 3:\n",
    "                    top_3_emotions.append(('none', 0))\n",
    "        \n",
    "                # Obliczanie średniej wartości emocji z wagami\n",
    "                emotion_weighted_average = 0.0\n",
    "                for i, (emotion, value) in enumerate(top_3_emotions):\n",
    "                    weight = emotion_intensity.get(emotion, 0)\n",
    "                    if i == 0:\n",
    "                        emotion_weighted_average += (value * weight * 0.60)\n",
    "                    elif i == 1:\n",
    "                        emotion_weighted_average += (value * weight * 0.25)\n",
    "                    elif i == 2:\n",
    "                        emotion_weighted_average += (value * weight * 0.15)\n",
    "                \n",
    "                emotion_scores.append(emotion_weighted_average)\n",
    "    \n",
    "            except Exception as e:\n",
    "                emotion_scores.append(0)\n",
    "    clip_emotion_score = create_clips_from_frames(emotion_scores, clip_length, skip)        \n",
    "    return clip_emotion_score\n",
    "\n",
    "\n",
    "def create_actor_video_clips(video_path, clip_length, skip):\n",
    "    frames = extract_frames_rgb(video_path)\n",
    "    return create_clips_from_frames(frames, clip_length, skip)\n",
    "    \n",
    "   \n",
    "\n",
    "#pomocnicze    \n",
    "def generate_final_video(selected_clips ,video_path ,output_folder=\"C:\\\\Users\\\\pwdlp\\\\OneDrive\\\\Pulpit\\\\Clips\"):\n",
    "        \"\"\"Tworzy osobne filmy dla każdego wybranego zakresu czasowego.\"\"\"\n",
    "    \n",
    "        if not selected_clips:\n",
    "            print(\"⚠️ Brak wybranych fragmentów! Nie można wygenerować filmów.\")\n",
    "            return\n",
    "    \n",
    "        # Otwieramy oryginalne wideo\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "        if not cap.isOpened():\n",
    "            print(\"❌ Nie udało się otworzyć pliku wideo!\")\n",
    "            return\n",
    "    \n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "        # Używamy kodeku 'mp4v' dla plików .mp4\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    \n",
    "        # Przetwarzanie każdej krotki w selected_clips\n",
    "\n",
    "        for clip_index, (start_sec, end_sec) in enumerate(selected_clips):\n",
    "            # Ścieżka do zapisu pliku wyjściowego\n",
    "            output_path = f\"{output_folder}\\\\clip_{clip_index}.mp4\"\n",
    "    \n",
    "            # Inicjalizacja VideoWriter dla nowego pliku\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "            if not out.isOpened():\n",
    "                print(f\"❌ Nie udało się utworzyć pliku wyjściowego: {output_path}\")\n",
    "                continue\n",
    "    \n",
    "            # Przeliczenie sekund na klatki\n",
    "            start_frame = int(start_sec * fps)\n",
    "            end_frame = int(end_sec * fps)\n",
    "    \n",
    "            # Ustawienie pozycji na początkową klatkę\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "            # Zapis klatek w zakresie\n",
    "            while cap.get(cv2.CAP_PROP_POS_FRAMES) < end_frame:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"❌ Błąd: Nie udało się odczytać klatki w klipie {clip_index}!\")\n",
    "                    break\n",
    "                out.write(frame)\n",
    "    \n",
    "            # Zamknięcie pliku wyjściowego\n",
    "            out.release()\n",
    "            print(f\"✅ Klip {clip_index} zapisano jako: {output_path}\")\n",
    "    \n",
    "        # Zwolnienie zasobów\n",
    "        cap.release() \n",
    "     \n",
    "#        \n",
    "        \n",
    "def extract_audio(video_path, sample_rate=16000):\n",
    "\n",
    "    # 1. Wczytaj plik wideo za pomocą PyDub\n",
    "    audio = AudioSegment.from_file(video_path)\n",
    "\n",
    "    # 2. Jeśli częstotliwość próbkowania nie jest zgodna z docelową, wykonaj resampling\n",
    "    if audio.frame_rate != sample_rate:\n",
    "        audio = audio.set_frame_rate(sample_rate)\n",
    "\n",
    "    # 3. Przekonwertuj dźwięk do numpy array\n",
    "    audio_array = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
    "\n",
    "    # 4. Jeśli dźwięk jest stereo, przekształć go na mono\n",
    "    if audio.channels > 1:\n",
    "        audio_array = audio_array.reshape(-1, audio.channels).mean(axis=1)\n",
    "\n",
    "    # 5. Normalizuj dźwięk do zakresu [-1, 1]\n",
    "    audio_array = audio_array / (2 ** (audio.sample_width * 8 - 1))\n",
    "\n",
    "\n",
    "    return audio_array   \n",
    "\n",
    "def create_clips_from_frames(frames, clip_length, skip=1):\n",
    "    return [frames[start:start + clip_length] for start in range(0, len(frames) - clip_length + 1, skip)]\n",
    "\n",
    "    "
   ],
   "id": "d9b6bd45442fb42b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "funkcja do pobierania i oceny właściwosci dźwięku\n",
   "id": "d282ad324f5b2f7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:52:10.562743Z",
     "start_time": "2025-03-02T21:52:10.547911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_audio_metrics(audio_array, sr):\n",
    "    \"\"\"\n",
    "    Wyciąga i normalizuje metryki audio, zwracając słownik.\n",
    "    \"\"\"\n",
    "\n",
    "    T = len(audio_array) / sr\n",
    "\n",
    "    # Minimalny czas trwania onsetu (w sekundach)\n",
    "    min_onset_duration = 0.1  # 100 ms\n",
    "\n",
    "    # Oblicz maksymalną liczbę onsetów\n",
    "    max_onsets = T / min_onset_duration\n",
    "\n",
    "    # Wyciąganie metryk audio\n",
    "    rms = librosa.feature.rms(y=audio_array)[0]\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio_array, sr=sr)[0]\n",
    "    energy = np.sum(audio_array**2) / len(audio_array)\n",
    "    onset_frames = librosa.onset.onset_detect(y=audio_array, sr=sr, units='frames')\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio_array)[0]\n",
    "    silence_ratio = np.mean(np.abs(audio_array) < 0.01)\n",
    "\n",
    "    # Normalizacja metryk do zakresu [0, 1]\n",
    "    rms_normalized = np.mean(rms)  # Już w zakresie [0, 1]\n",
    "    spectral_centroid_normalized = np.mean(spectral_centroid) / (sr / 2)  # Znormalizowane\n",
    "    energy_normalized = energy  # Już w zakresie [0, 1]\n",
    "    onset_count_normalized = len(onset_frames) / max_onsets  # Znormalizowane\n",
    "    zcr_normalized = np.mean(zcr) / 0.5  # Znormalizowane\n",
    "    silence_score = 1 - silence_ratio  # Już w zakresie [0, 1]\n",
    "\n",
    "    # Wyciąganie metryk i tworzenie słownika\n",
    "    metrics = {\n",
    "        \"rms_mean\": rms_normalized ,\n",
    "        \"spectral_std\": spectral_centroid_normalized,\n",
    "        \"energy_log\": energy_normalized,\n",
    "        \"onset_count\": onset_count_normalized,\n",
    "        \"zcr_mean\": zcr_normalized,\n",
    "        \"silence_score\": silence_score\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_clip_audio_score(metrics):\n",
    "    weights = {\n",
    "        \"rms_mean\": 0.3,\n",
    "        \"spectral_std\": 0.05,\n",
    "        \"energy_log\": 0.2,\n",
    "        \"onset_count\": 0.4,\n",
    "        \"zcr_mean\": 0.03,\n",
    "        \"silence_score\": 0.02\n",
    "    }\n",
    "    score = sum(metrics[key] * weight for key, weight in weights.items())\n",
    "    \n",
    "    return score\n",
    "\n",
    "def split_audio_into_clips(audio_array, sample_rate, clip_duration, total_duration, skip):\n",
    "    clips = [\n",
    "        (\n",
    "            audio_array[int(start * sample_rate):int((start + clip_duration) * sample_rate)],\n",
    "            sample_rate\n",
    "        )\n",
    "        for start in np.arange(0, total_duration - clip_duration + skip, skip)\n",
    "        if (start + clip_duration) <= total_duration\n",
    "    ]\n",
    "    return clips\n",
    "\n",
    "def calculate_audio_score(video_path, clip_duration, skip, total_duration, sample_rate=22050):\n",
    "    # 1. Ekstrahuj cały dźwięk\n",
    "    audio_array = extract_audio(video_path, sample_rate)\n",
    "\n",
    "    # 2. Przygotuj listę przedziałów czasowych do analizy\n",
    "    clip_times = split_audio_into_clips(audio_array, sample_rate, clip_duration, total_duration, skip)\n",
    "\n",
    "    # 3. Oblicz score dla każdego fragmentu\n",
    "    metrics = [extract_audio_metrics(*clip) for clip in clip_times]\n",
    "    scores = [calculate_clip_audio_score(metrics) for metrics in metrics]\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def create_actor_audio_clips(video_path, clip_duration, skip, total_duration, sample_rate=22050):\n",
    "    audio_array = extract_audio(video_path, sample_rate)\n",
    "    clip_times = split_audio_into_clips(audio_array, sample_rate, clip_duration, total_duration, skip)\n",
    "    metrics = [list(extract_audio_metrics(*clip).values()) for clip in clip_times]\n",
    "    return np.array(metrics)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ],
   "id": "729f3569df433939",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "funkcje do przetwarzania dźwięku na tekst i jego oceny",
   "id": "cd73a1d53c212709"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:52:10.593115Z",
     "start_time": "2025-03-02T21:52:10.580087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def audio_to_text(audio_array, sample_rate=16000, device=\"cuda\"):\n",
    "    if not isinstance(audio_array, np.ndarray):\n",
    "        raise ValueError(\"audio_array should be a numpy array\")\n",
    "\n",
    "    # Upewnij się, że częstotliwość próbkowania jest zgodna z wymaganiami Whispera\n",
    "    if sample_rate != 16000:\n",
    "        raise ValueError(\"Whisper expects audio with a sample rate of 16000 Hz\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Załaduj model Whisper i przenieś go na GPU\n",
    "    model = whisper.load_model(\"base\").to(device)\n",
    "\n",
    "    result = model.transcribe(audio_array, fp16=False)\n",
    "\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "#do przekminienia (najlepiej opierdolic to tą drugą funnkcją do podziału zeby była jedna)\n",
    "def split_text_by_time(text, segment_duration=20):\n",
    "    \"\"\"\n",
    "    Funkcja dzieli tekst na fragmenty o zadanej długości w sekundach\n",
    "    (segment_duration to długość w sekundach).\n",
    "    \"\"\"\n",
    "    # W tym przykładzie zakładajmy, że mamy podzielony tekst na linie\n",
    "    # Możesz podzielić w zależności od swojej struktury\n",
    "    lines = text.split(\"\\n\")\n",
    "    segments = []\n",
    "    \n",
    "    current_segment = []\n",
    "    current_duration = 0\n",
    "\n",
    "    # Załóżmy, że każda linia trwa około 2 sekund (możesz dostosować)\n",
    "    line_duration = 2 \n",
    "\n",
    "    for line in lines:\n",
    "        current_segment.append(line)\n",
    "        current_duration += line_duration\n",
    "        if current_duration >= segment_duration:\n",
    "            segments.append(\" \".join(current_segment))\n",
    "            current_segment = []\n",
    "            current_duration = 0\n",
    "\n",
    "    # Dodajemy ostatni fragment, jeśli pozostały linie\n",
    "    if current_segment:\n",
    "        segments.append(\" \".join(current_segment))\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "def analyze_text_with_roberta(text):\n",
    "    # Analizowanie sentymentu\n",
    "    sentiment = sentiment_model(text)\n",
    "    \n",
    "    # Analiza emocji\n",
    "    emotion_result = emotion_model(text, candidate_labels=[\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"disgust\"])\n",
    "    \n",
    "    # Analiza tematu\n",
    "    topics = [\"politics\", \"sports\", \"entertainment\", \"technology\", \"science\", \"health\"]\n",
    "    topic_result = topic_model(text, candidate_labels=topics)\n",
    "    \n",
    "    # Możesz dodać inne analizy, np. złożoność tekstu itp.\n",
    "\n",
    "    # Wagi dla wyników analizy\n",
    "    weights = {\n",
    "        \"sentiment\": 0.2,\n",
    "        \"emotion\": 0.3,\n",
    "        \"topic\": 0.2\n",
    "        # Dodaj wagi dla innych analiz\n",
    "    }\n",
    "\n",
    "    # Przypisujemy wagi do wyników i obliczamy końcowy score\n",
    "    score = (\n",
    "        weights[\"sentiment\"] * sentiment[0]['score'] +\n",
    "        weights[\"emotion\"] * emotion_result['scores'][0] +\n",
    "        weights[\"topic\"] * topic_result['scores'][0]\n",
    "    )\n",
    "\n",
    "    return score, [sentiment, emotion_result, topic_result]\n",
    "\n",
    "\n",
    "def calculate_scores_for_video(video_path, segment_duration=20):\n",
    "    audio, duration = extract_audio(video_path)\n",
    "    text = audio_to_text(audio)\n",
    "    segments = split_text_by_time(text, segment_duration)\n",
    "    result_metrics = [analyze_text_with_roberta(segment) for segment in segments]\n",
    "    scores, metrics = zip(*result_metrics)\n",
    "    return scores, metrics"
   ],
   "id": "45fe4ec72b529f97",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:52:10.671736Z",
     "start_time": "2025-03-02T21:52:10.659387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def skewed_gaussian_weights(length):\n",
    "    \"\"\"\n",
    "    Generuje wagi, które:\n",
    "    - Od 0 do 0.6 rosną szybko, nieliniowo (kwadratowo), z zaokrągleniem w okolicach środka.\n",
    "    - Od 0.6 do 0.8 bardzo powoli spadają (kwadratowo).\n",
    "    - Od 0.8 do 1 gwałtownie spadają liniowo do 0.\n",
    "    \"\"\"\n",
    "    # Tworzymy zakres x od 0 do 1\n",
    "    x = np.linspace(0, 1, length)\n",
    "    \n",
    "    # Inicjalizacja wag\n",
    "    weights = np.zeros(length)\n",
    "    \n",
    "    # 1. Od 0 do 0.6: szybki wzrost nieliniowy (kwadratowy) z zaokrągleniem\n",
    "    mask_rise = x <= 0.6\n",
    "    x_rise = x[mask_rise] / 0.6  # Normalizacja do zakresu [0, 1]\n",
    "    weights[mask_rise] = 0.3 + 0.7 * (x_rise ** 2)  # Kwadratowy wzrost\n",
    "    \n",
    "    # 2. Od 0.6 do 0.8: bardzo powolny spadek nieliniowy (kwadratowy)\n",
    "    mask_gentle_fall = (x > 0.6) & (x <= 0.8)\n",
    "    x_fall = (x[mask_gentle_fall] - 0.6) / 0.2  # Normalizacja do zakresu [0, 1]\n",
    "    weights[mask_gentle_fall] = 1.0 - 0.5 * (x_fall ** 2)  # Kwadratowy spadek\n",
    "    \n",
    "    # 3. Od 0.8 do 1: gwałtowny spadek liniowy do 0\n",
    "    mask_sharp_fall = x > 0.8\n",
    "    x_sharp_fall = (x[mask_sharp_fall] - 0.8) / 0.2  # Normalizacja do zakresu [0, 1]\n",
    "    weights[mask_sharp_fall] = 0.5 - 0.5 * x_sharp_fall  # Liniowy spadek\n",
    "    \n",
    "    # Normalizacja wag\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "class VideoClipEnv(gym.Env):\n",
    "    def __init__(self, clips, audio_metrics, clip_length, video_scores, audio_scores):\n",
    "        super(VideoClipEnv, self).__init__()\n",
    "        \n",
    "        #operujemy na liczbie klatek ponieważ jedna klatka tożsama jest z jedną sekundą w tym podejściu\n",
    "        self.clips = clips  # Lista klipów (każdy klip to tablica klatek)\n",
    "        self.audio_metrics = audio_metrics  # Lista metryk dźwięku (dla każdego klipu)\n",
    "        # self.text_metrics = text_metrics  # Lista metryk tekstu (dla każdego klipu\n",
    "        self.total_number_of_clips = len(self.clips)\n",
    "        \n",
    "        self.video_emotion_scores = video_scores\n",
    "        self.audio_emotion_scores = audio_scores\n",
    "        # self.text_emotion_scores = text_scores\n",
    "\n",
    "        \n",
    "\n",
    "        # Przestrzeń akcji: 0 (pomiń), 1 (zachowaj)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.clip_length = clip_length\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"video\": spaces.Box(low=0, high=255, shape=(self.clip_length, 1080, 1920, 3), dtype=np.uint8),  # Klipy\n",
    "            \"audio_metrics\": spaces.Box(low=0, high=1, shape=(self.clip_length, 6), dtype=np.float32),  # Metryki dźwięku\n",
    "            # \"text_metrics\": spaces.Box(low=0, high=1, shape=(self.clip_length, 3), dtype=np.float32),  # Metryki tekstu\n",
    "        })\n",
    "        \n",
    "        self.step_length = 1\n",
    "\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        info = {}  # Dodaj pusty słownik info\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        reward = self._calculate_reward() if action == 1 else 0\n",
    "\n",
    "        done = self.current_step > self.total_number_of_clips\n",
    "        \n",
    "        self.current_step += self.step_length \n",
    "\n",
    "    \n",
    "        return observation, reward, done, False, {}\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \n",
    "        clip = self.clips[self.current_step]\n",
    "        audio = self.audio_metrics[self.current_step]\n",
    "        # text = self.text_metrics[self.current_step]\n",
    "    \n",
    "        return {\n",
    "            \"video\": np.array(clip, dtype=np.uint8),\n",
    "            \"audio_metrics\": np.array(audio, dtype=np.float32),\n",
    "            # \"text_metrics\": np.array(text, dtype=np.float32),\n",
    "        }\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        emotion_score = self._detect_emotion()\n",
    "        audio_score = self._calculate_audio_score()\n",
    "\n",
    "        reward = emotion_score * 0.3 + audio_score * 0.7\n",
    "    \n",
    "        return reward\n",
    "    \n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # print(f\"Step Render: {self.current_step}, Selected Clips: {self.selected_clips}\")\n",
    "        # print(f\"Emotions: {self._detect_emotion([self.frames[self.current_step]])}\")\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    #funkcje dodatkowe\n",
    "    def _detect_emotion(self):\n",
    "        scores = np.array(self.video_emotion_scores[self.current_step])\n",
    "    \n",
    "        weights = skewed_gaussian_weights(len(scores))\n",
    "\n",
    "        return np.sum(scores * weights)\n",
    "    def _calculate_audio_score(self):\n",
    "        score = np.array(self.audio_emotion_scores[self.current_step])\n",
    "\n",
    "        \n",
    "        return np.sum(score)\n",
    "\n"
   ],
   "id": "53e831a87bf7aa0b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:52:10.718142Z",
     "start_time": "2025-03-02T21:52:10.704161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class CustomEfficientNetCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=1024):\n",
    "        super(CustomEfficientNetCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        # Warstwa wejściowa\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),  # 1920x1080 -> 960x540\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Bloki MBConv (inspirowane EfficientNet)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            self._make_mbconv_block(32, 16, stride=1),  # 960x540 -> 960x540\n",
    "            self._make_mbconv_block(16, 24, stride=2),  # 960x540 -> 480x270\n",
    "            self._make_mbconv_block(24, 40, stride=2),  # 480x270 -> 240x135\n",
    "            self._make_mbconv_block(40, 80, stride=2),  # 240x135 -> 120x68\n",
    "            self._make_mbconv_block(80, 112, stride=2),  # 120x68 -> 60x34\n",
    "            self._make_mbconv_block(112, 192, stride=2),  # 60x34 -> 30x17\n",
    "        )\n",
    "        \n",
    "        # Warstwa spłaszczająca\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Warstwa liniowa do dopasowania do wymaganego wymiaru cech (1024)\n",
    "        self.fc = nn.Linear(192 * 30 * 17, features_dim)\n",
    "    \n",
    "    def _make_mbconv_block(self, in_channels, out_channels, stride):\n",
    "        # Blok MBConv z rozszerzeniem i redukcją kanałów\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * 6, kernel_size=1, stride=1, padding=0, bias=False),  # Rozszerzenie\n",
    "            nn.BatchNorm2d(in_channels * 6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels * 6, in_channels * 6, kernel_size=3, stride=stride, padding=1, groups=in_channels * 6, bias=False),  # Depthwise\n",
    "            nn.BatchNorm2d(in_channels * 6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels * 6, out_channels, kernel_size=1, stride=1, padding=0, bias=False),  # Redukcja\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        # Pobierz dane wideo\n",
    "        video = observations[\"video\"]  # Kształt: (batch_size, liczba_klatek, wysokość, szerokość, kanały)\n",
    "        \n",
    "        # Przekształć kształt na (batch_size * liczba_klatek, kanały, wysokość, szerokość)\n",
    "        batch_size, num_frames, height, width, channels = video.shape\n",
    "        video = video.view(batch_size * num_frames, height, width, channels)  # (batch_size * num_frames, height, width, channels)\n",
    "        video = video.permute(0, 3, 1, 2)  # (batch_size * num_frames, channels, height, width)\n",
    "        \n",
    "        # Przetwórz klatki wideo przez warstwę wejściową\n",
    "        x = self.initial_conv(video)\n",
    "        \n",
    "        # Przetwórz przez bloki MBConv\n",
    "        x = self.conv_blocks(x)\n",
    "        \n",
    "        # Spłaszcz i przetwórz przez warstwę liniową\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Przywróć oryginalny kształt batcha\n",
    "        x = x.view(batch_size, num_frames, -1)  # (batch_size, num_frames, features_dim)\n",
    "        \n",
    "        # Uśrednij cechy po klatkach\n",
    "        x = x.mean(dim=1)  # (batch_size, features_dim)\n",
    "        \n",
    "        return self.fc(x)"
   ],
   "id": "fdad847f4cec1cb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T21:53:22.339893Z",
     "start_time": "2025-03-02T21:52:10.735140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_path = \"C:\\\\Users\\\\pwdlp\\\\OneDrive\\\\Pulpit\\\\Vods\\\\sperma.mp4\"\n",
    "clip_duration = 10\n",
    "skip = 1\n",
    "sample_rate = 22050\n",
    "\n",
    "frames  = extract_frames_rgb(test_path)\n",
    "\n",
    "\n",
    "#video clips and scores\n",
    "clips_for_actor = create_clips_from_frames(frames, clip_duration, skip)\n",
    "total_duration = len(clips_for_actor)\n",
    "\n",
    "emotion_score = calculate_video_score(frames, clip_duration, skip)\n",
    "\n",
    "#tekst\n",
    "\n",
    "# text_score, actor_text =  calculate_scores_for_video(test_path, clip_duration)\n",
    "\n",
    "#audio clips and scores\n",
    "audio_score = calculate_audio_score(test_path, clip_duration, skip, total_duration, sample_rate=sample_rate)\n",
    "audio_for_actor = create_actor_audio_clips(test_path, clip_duration, skip, total_duration, sample_rate=sample_rate)\n",
    "\n",
    "# print(audio_for_actor[0])\n",
    "\n"
   ],
   "id": "d4bd9df246a784b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic fps:  25\n",
      "critic 1st frame (1080, 1920, 3)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T19:12:59.751743Z",
     "start_time": "2025-03-02T19:12:45.773132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomEfficientNetCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=1024),\n",
    ")\n",
    "\n",
    "\n",
    "env = VideoClipEnv(clips_for_actor, audio_for_actor, clip_duration, emotion_score, audio_score)\n",
    "env = RecordEpisodeStatistics(env)\n",
    "\n",
    "model = A2C(\n",
    "    MultiInputActorCriticPolicy,\n",
    "    env=env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    n_steps=30,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    device=device,\n",
    ")\n",
    " \n",
    "model.learn(total_timesteps=100)"
   ],
   "id": "b6a2cd39d8ca878a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    201\u001B[0m     obs_tensor \u001B[38;5;241m=\u001B[39m obs_as_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 202\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\stable_baselines3\\common\\policies.py:645\u001B[0m, in \u001B[0;36mActorCriticPolicy.forward\u001B[1;34m(self, obs, deterministic)\u001B[0m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;66;03m# Preprocess the observation if needed\u001B[39;00m\n\u001B[1;32m--> 645\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001B[0m, in \u001B[0;36mActorCriticPolicy.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[1;32m--> 672\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\stable_baselines3\\common\\policies.py:131\u001B[0m, in \u001B[0;36mBaseModel.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    130\u001B[0m preprocessed_obs \u001B[38;5;241m=\u001B[39m preprocess_obs(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space, normalize_images\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnormalize_images)\n\u001B[1;32m--> 131\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfeatures_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocessed_obs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[60], line 54\u001B[0m, in \u001B[0;36mCustomEfficientNetCNN.forward\u001B[1;34m(self, observations)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# Przetwórz przez bloki MBConv\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_blocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Spłaszcz i przetwórz przez warstwę liniową\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[0;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[0;32m    548\u001B[0m     )\n\u001B[1;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.87 GiB is allocated by PyTorch, and 117.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[65], line 26\u001B[0m\n\u001B[0;32m     12\u001B[0m env \u001B[38;5;241m=\u001B[39m RecordEpisodeStatistics(env)\n\u001B[0;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m A2C(\n\u001B[0;32m     15\u001B[0m     MultiInputActorCriticPolicy,  \u001B[38;5;66;03m# Używamy MultiInputPolicy\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     env\u001B[38;5;241m=\u001B[39menv,  \u001B[38;5;66;03m# Twoje środowisko\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,  \u001B[38;5;66;03m# Urządzenie (CPU/GPU)\u001B[39;00m\n\u001B[0;32m     24\u001B[0m )\n\u001B[1;32m---> 26\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001B[0m, in \u001B[0;36mA2C.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfA2C,\n\u001B[0;32m    194\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    199\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    200\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfA2C:\n\u001B[1;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 323\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    326\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\AIPythonLab\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001B[39;00m\n\u001B[0;32m    201\u001B[0m     obs_tensor \u001B[38;5;241m=\u001B[39m obs_as_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 202\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    205\u001B[0m \u001B[38;5;66;03m# Rescale and perform action\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "selected_clips = []  # Lista na wybrane klipy\n",
    "\n",
    "env_original = env.env\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)  # Wybieramy akcję\n",
    "    next_obs, reward, done, truncated, info = env.step(action)  # Wykonaj krok w środowisku\n",
    "    if action == 1:\n",
    "        selected_clips.append((env_original.current_step, env_original.current_step + env_original.clip_length))\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "env.close()\n",
    "\n"
   ],
   "id": "57a3d400e0ebe9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "testowanie",
   "id": "21aed5e0068dc771"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
